import logging
import os
import numpy as np
import pandas as pd

from opencell.database import utils, constants

logger = logging.getLogger(__name__)


def parseFloat(val):
    try:
        val = float(val)
    except ValueError:
        val = float(str(val).replace(',', ''))
    return val


def load_library_snapshot(filename):
    '''
    Load and format a CSV 'snapshot' of a library spreadsheet

    These 'snapshots' are of the google sheet created/maintained by Manu
    that contains the crispr designs for all plates
    '''
    # define maps from the column names in the google sheets
    # to the column names in the models.metadata.CrisprDesign table
    # (all required columns are included, even if their name is unchanged)

    # column names in the original 'library' google sheet (containing plates 1-22)
    library_columns = {
        'plate_id': 'plate_id',
        'well_id': 'well_id',

        'gene_name': 'target_name',
        'family': 'target_family',

        'enst_id': 'enst_id',
        'terminus_to_tag': 'target_terminus',

        'protospacer_name': 'protospacer_name',
        'protospacer_note': 'protospacer_notes',
        'protospacer_sequence': 'protospacer_sequence',

        'ultramer_name': 'template_name',
        'ultramer_note': 'template_notes',
        'ultramer_sequence': 'template_sequence',
    }

    # alternative column names for some columns, specific to the 'library v1.1' spreadsheet
    # (this sheet starts with plate23 in Jan 2022)
    library_columns_2022 = {
        'tagged_terminus': 'target_terminus',
        'ensembl_transcript_id': 'enst_id',
        'gRNA_sequence': 'protospacer_sequence',
        'donor_sequence': 'template_sequence',
    }

    library_columns.update(library_columns_2022)

    library = pd.read_csv(filename)
    library.rename(columns=library_columns, inplace=True)

    # for clarity, format the plate_ids here
    library['plate_id'] = library.plate_id.apply(utils.format_plate_design_id)

    # drop any extraneous columns
    dropped_columns = list(set(library.columns).difference(library_columns.values()))
    library = library.drop(labels=dropped_columns, axis=1)

    return library


def load_electroporation_history(filename):
    '''
    Load and format a 'snapshot' of the list of electroporations
    (this is a google sheet from Manu)

    Expected columns: ('plate_id', 'date', 'notes')
    '''

    electroporation_columns = {
        'plate_id': 'plate_id',
        'electroporation_date': 'date',
        'comment': 'notes',
    }
    electroporations = pd.read_csv(filename)
    electroporations.rename(columns=electroporation_columns, inplace=True)

    # format the plate_id
    electroporations['plate_id'] = electroporations.plate_id.apply(
        utils.format_plate_design_id)

    # drop unneeded columns
    electroporations = electroporations[list(electroporation_columns.values())]

    return electroporations


def load_legacy_microscopy_master_key(filepath):
    '''
    Load and format a snapshot of the 'legacy' tab of the 'Pipeline-microscopy-master-key' google sheet

    These are all pipeline-related microscopy acquisitions prior to the transition to PML-based IDs
    (which occurred at ML0196).

    Note that these acquisitions have IDs of the form 'MLxxxx_YYYYMMDD'.
    '''

    md = pd.read_csv(filepath)
    md = md.rename(columns={c: c.replace(' ', '_').lower() for c in md.columns})

    md = md.rename(columns={
        'id': 'legacy_id',
        'automated_acquisition?': 'automation',
        'acquisition_notes': 'notes',
        'primary_imager': 'imager',
    })

    md = md.drop(labels=[c for c in md.columns if c.startswith('unnamed')], axis=1)

    # separate the ML ID itself from the date
    md['ml_id'] = md.legacy_id.apply(lambda s: s.split('_')[0])

    # parse the date from the ML-style ID
    md['date'] = pd.to_datetime(md.legacy_id.apply(lambda s: s.split('_')[1]))

    # prepend the P to create the PML-style ID
    md['pml_id'] = [f'P{ml_id}' for ml_id in md.ml_id]

    # columns to retain
    md = md[['pml_id', 'date', 'automation', 'imager', 'description', 'notes']]
    return md


def load_pipeline_microscopy_master_key(filepath):
    '''
    Load a CSV snapshot of the 'pipeline-microscopy-master-key' google sheet
    '''
    snapshot = pd.read_csv(filepath)
    snapshot.rename(columns={'id': 'pml_id'}, inplace=True)
    snapshot.dropna(how='any', subset=['pml_id', 'date'], axis=0, inplace=True)
    return snapshot


def load_pipeline_microscopy_dataset_metadata(filepath):
    '''
    Load the 'fov-metadata.csv' file from a PML dataset directory
    (this file is generated by dragonfly_automation.qc.pipeline_plate_qc.construct_fov_metadata)

    This method drops rows for which the manually_flagged column is set to True
    and constructs the raw_filepath column. Other than that, it does no data validation.
    We rely on the implicit validation that occurs when the dataframe returned by this method
    is used to populate the MicroscopyFOV table in PolyclonalLineOperations.insert_microscopy_fovs
    '''
    metadata = pd.read_csv(filepath)

    # drop rows that were manually flagged
    if np.any(metadata.manually_flagged):
        logger.warning('Dropping %s manually flagged FOVs' % metadata.manually_flagged.sum())
        metadata = metadata.loc[~metadata.manually_flagged]

    # the filepath to the raw TIFF file
    metadata['raw_filepath'] = [
        os.path.join(row.src_dirpath, row.src_filename) for ind, row in metadata.iterrows()
    ]
    # infer the sort_count column for datasets that predate the introduction of resorting
    if 'sort_count' not in metadata.columns:
        logger.warning(
            'There is no sort_count column in the FOV metadata so a sort_count of 1 will be used'
        )
        metadata['sort_count'] = 1
    return metadata


def read_and_validate_platemap(filepath):
    '''
    A 'platemap' is a CSV of well_ids and target_names
    for a single pipeline plate. Here, we load such a CSV,
    assign column names, and do some quick sanity checks.

    ***WARNING***
    This method *assumes* that the well_id appears in the first column
    and the target name appears in the second column.
    '''
    platemap = pd.read_csv(filepath, header=None)
    platemap.columns = ['well_id', 'target_name']

    # check for missing/unexpected well_ids
    missing_well_ids = set(constants.RAW_WELL_IDS).difference(platemap.well_id)
    if missing_well_ids:
        logger.warning('Some well_ids are missing: %s' % missing_well_ids)

    unexpected_well_ids = set(platemap.well_id).difference(constants.RAW_WELL_IDS)
    if unexpected_well_ids:
        logger.warning('Unexpected well_ids %s' % unexpected_well_ids)

    # check for missing target (gene) names
    if pd.isna(platemap.target_name).sum():
        raise ValueError('Some target names are missing in platemap %s' % filepath)

    return platemap
